{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.80      0.79       952\n",
      "         1.0       0.41      0.38      0.39       352\n",
      "\n",
      "    accuracy                           0.68      1304\n",
      "   macro avg       0.59      0.59      0.59      1304\n",
      "weighted avg       0.68      0.68      0.68      1304\n",
      "\n",
      "TestF1: 0.5888260035379691\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load evaluate.py\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from torch.nn import *\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.model import ImgSpecModel\n",
    "from dataloaderraw import ImageSpecDataset\n",
    "\n",
    "# combined model\n",
    "imgspec_model = ImgSpecModel()\n",
    "\n",
    "# change model path to the path of saved model you want to verify\n",
    "model_path = 'save_50_epoch_f1_score\\\\model_best_acc_33_0.586962471491.pth'\n",
    "imgspec_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "test_imgspec_dataset = ImageSpecDataset(root_dir='small_dataset_objects', split='test')\n",
    "test_dataloader = DataLoader(test_imgspec_dataset, batch_size=10, shuffle=True, num_workers=4)\n",
    "\n",
    "class_weight = torch.from_numpy(np.array([1.0, 3.0])).float()\n",
    "cce_loss = CrossEntropyLoss(weight=class_weight)\n",
    "\n",
    "# test model performance\n",
    "imgspec_model.eval()\n",
    "correct_class, incorrect_class = 0, 0\n",
    "all_preds, all_org = [], []\n",
    "for i_batch, batch in enumerate(test_dataloader):\n",
    "    img_feats = batch['img_feats'].float()\n",
    "    spec_imgs = batch['spec'].unsqueeze(1).float()\n",
    "    scores = batch['score'].float().numpy()\n",
    "    preds = F.softmax(imgspec_model(img_feats, spec_imgs).detach(), dim=1)\n",
    "    _, preds = torch.max(preds, 1)\n",
    "    preds = preds.cpu().numpy()\n",
    "    #preds = np.where(preds > 0.5, 1, 0)\n",
    "    for org, pred in zip(scores, preds):\n",
    "        correct_class += int(org == pred)\n",
    "        incorrect_class += int(org != pred)\n",
    "        all_org.append(org)\n",
    "        all_preds.append(pred)\n",
    "        \n",
    "print('\\n\\n')\n",
    "print(classification_report(all_org, all_preds))\n",
    "acc = f1_score(all_org, all_preds, average='macro')\n",
    "print('TestF1: {}'.format(acc))\n",
    "print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
