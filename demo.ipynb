{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImgSpecModel(\n",
      "  (spec_layers): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (reduction): Sequential(\n",
      "    (0): Linear(in_features=14400, out_features=2048, bias=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# %load demo.py\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from shutil import copyfile\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from torch.nn import *\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.model import ImgSpecModel\n",
    "from dataloaderraw import ImageSpecDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_demo(root_dir, img_name, demo_out_dir):\n",
    "    lines = open(os.path.join(root_dir, 'test_split.tsv'), 'r').readlines()\n",
    "\n",
    "    obj_list = []\n",
    "    for line in lines:\n",
    "        line_split = line.split('\\t')\n",
    "        if line_split[1] == img_name:\n",
    "            obj_list.append(line_split[0])\n",
    "\n",
    "    img_spec_list = []\n",
    "    for i in range(len(obj_list)):\n",
    "        for j in range(len(obj_list)):\n",
    "            img_spec_list.append((obj_list[i], obj_list[j]))\n",
    "\n",
    "    # combined model\n",
    "    imgspec_model = ImgSpecModel()\n",
    "\n",
    "    imgspec_model.load_state_dict(torch.load('save_50_epoch_f1_score\\\\model_best_acc_33_0.586962471491.pth',map_location=torch.device('cpu')))\n",
    "\n",
    "    img_spec_list_w_scores = []\n",
    "    for img, spec in img_spec_list:\n",
    "        img_name = os.path.join(root_dir, 'test', 'img_feats', img.split('.')[0]+'.npy')\n",
    "        img_feats = torch.from_numpy(np.load(img_name)).float().unsqueeze(0)\n",
    "        spec_name = os.path.join(root_dir, 'test', 'spec', spec.split('.')[0]+'.png')\n",
    "        spec_img = torch.from_numpy(io.imread(spec_name, as_gray=True) / 255.0)\n",
    "        spec_img = spec_img.unsqueeze(0).unsqueeze(1).float()\n",
    "        pred = F.softmax(imgspec_model(img_feats, spec_img).detach(), dim=1).detach().cpu().numpy()\n",
    "        img_spec_list_w_scores.append((img, spec, pred[0][1]))\n",
    "\n",
    "    img_spec_list_w_scores.sort(key=lambda x: -x[2])\n",
    "    used_img, used_spec = {}, {}\n",
    "\n",
    "    file_cnt = 0\n",
    "    for img, spec, score in img_spec_list_w_scores:\n",
    "        if img not in used_img and spec not in used_spec and score >= 0.5:\n",
    "            file_cnt += 1\n",
    "            used_img[img] = True\n",
    "            used_spec[spec] = True\n",
    "            out_img = os.path.join(demo_out_dir, '{}.jpg'.format(file_cnt))\n",
    "            out_wav = os.path.join(demo_out_dir, '{}.wav'.format(file_cnt))\n",
    "            in_img = os.path.join(root_dir, 'test', 'img', '{}.jpg'.format(img))\n",
    "            in_wav = os.path.join(root_dir, 'test', 'wav', '{}.wav'.format(img))\n",
    "            copyfile(in_img, out_img)\n",
    "            copyfile(in_wav, out_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'small_dataset_objects'\n",
    "img_name = 'COCO_val2014_000000332653.jpg'\n",
    "demo_out_dir = 'new_dir1'\n",
    "\n",
    "run_demo(root_dir, img_name, demo_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
